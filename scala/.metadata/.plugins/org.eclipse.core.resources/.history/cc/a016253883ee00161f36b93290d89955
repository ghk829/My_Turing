package recommend
import java.io.File

import scala.io.Source

import org.apache.log4j.Logger
import org.apache.log4j.Level

import org.apache.spark.SparkConf
import org.apache.spark.SparkContext
import org.apache.spark.SparkContext._
import org.apache.spark.rdd._
import org.apache.spark.mllib.recommendation.{ALS, Rating, MatrixFactorizationModel}

object test {

  def main(args: Array[String]){

    Logger.getLogger("org.apache.spark").setLevel(Level.WARN)
    Logger.getLogger("org.eclipse.jetty.server").setLevel(Level.OFF)

    // set up environment

    val conf = new SparkConf()
      .setAppName("MovieLensALS").setMaster("local[*]")
      .set("spark.executor.memory", "2g")
    val sc = new SparkContext(conf)

    // load personal ratings
     val movieLensHomeDir = "C:/Users/evalue/git/movie_lens/data"
     
      val myRatingsRDD = sc.textFile(new File(movieLensHomeDir, "myratings.txt").toString).map { line =>
      val fields = line.split("|")
      // format: (movieId, movieName)
      (fields(0).toInt, fields(1))
    }.collect().toMap
    // load ratings and movie titles

   

    val ratings = sc.textFile(new File(movieLensHomeDir, "u.data").toString).map { line =>
      val fields = line.split("\t")
      // format: (timestamp % 10, Rating(userId, movieId, rating))
      (fields(3).toLong % 10, Rating(fields(0).toInt, fields(1).toInt, fields(2).toDouble))
    }

    val movies = sc.textFile(new File(movieLensHomeDir, "u.item").toString).map { line =>
      val fields = line.split("|")
      // format: (movieId, movieName)
      (fields(0).toInt, fields(1))
    }.collect().toMap

    val numRatings = ratings.count
    val numUsers = ratings.map(_._2.user).distinct.count
    val numMovies = ratings.map(_._2.product).distinct.count

    println("Got " + numRatings + " ratings from "
      + numUsers + " users on " + numMovies + " movies.")
    val numPartitions = 4
    val training = ratings.filter(x => x._1 < 6)
      .values
      .union(myRatingsRDD)
      .repartition(numPartitions)
      .cache()
    val validation = ratings.filter(x => x._1 >= 6 && x._1 < 8)
      .values
      .repartition(numPartitions)
      .cache()
    val test = ratings.filter(x => x._1 >= 8).values.cache()

    val numTraining = training.count()
    val numValidation = validation.count()
    val numTest = test.count()

    println("Training: " + numTraining + ", validation: " + numValidation + ", test: " + numTest)
    // clean up
    sc.stop()
  }
}